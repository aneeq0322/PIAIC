{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkja83E9FcnE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.__version__ < \"2.0.0\":\n",
    "  !pip install --upgrade tensorflow_gpu==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gda5jRovFiW0",
    "outputId": "2f20f6dd-a62f-44d6-a78c-1b777b9e9940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AR86319uF4Xo"
   },
   "source": [
    "# STEP 1 : Loading Data \n",
    "\n",
    "**The MNIST dataset comes preloaded in Keras, in the form of a set of four Numpy arrays.**\n",
    "\n",
    "**x_train and x_test parts contain greyscale RGB codes (from 0 to 255) .**\n",
    "\n",
    "**y_train and y_test parts contains labels from 0 to 9 which represents which number they actually are.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLUtVo5nF2hj"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7jbYAhkAF938",
    "outputId": "3804b21b-fd82-4b93-80aa-ba8b7cede695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1 so we will re-download the data.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 141s 12us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzbrhaPZGFU6"
   },
   "source": [
    "# STEP 2 : DATA PREPROCESSING\n",
    "\n",
    "We have to scale x_train and x_test so that all values are in the [0, 1] interval. **Why do we need to scale ?** goal of scaling is to speed up the algorithm's computation . It is used to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. It is also required for some algorithms to model the data correctly.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6VBnJ93GBYz"
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0  ,  x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aiVuJpU3GOef"
   },
   "source": [
    "# STEP 3 : BUILD NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G1iXzKwGE8f"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKqXFd79Gn94"
   },
   "source": [
    "# STEP 4 : COMPILATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVJ9fFM-GTBx"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fp3f6yIoHRmo"
   },
   "source": [
    "# STEP 5 : TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "XrcQAPArHA4L",
    "outputId": "a175270f-0697-4b0f-aff3-d75fc3200924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 90s 1ms/sample - loss: 0.0675 - accuracy: 0.9787 1:31 - loss: 0.068 - ETA: 1:29 - loss: 0.0685 - accuracy: 0. - ETA: 1:30 - loss: 0.0688  - ETA:  - ETA: 1:25 - loss: - - ETA: 1:17 - loss: 0.0629 - accuracy: 0.98 - - ETA: 1:15 - loss: 0.0623 - ac - ETA: 1:15 - loss: 0.0610  - ETA: 1:14 - loss: 0.0610 - accu - ETA: 1:13 - loss: 0.060 - ETA: 1:14 - l - ETA: 1:11 - loss: 0.0615 - accuracy: 0. - ETA: 1:11 - loss: 0.061 - ETA: 1:11 - loss: 0.0626 - accuracy: 0. - ETA: 1:11 - loss: 0.0628 - accura - ETA: 1:10 - loss: 0.0623 - accuracy:  - ETA: 1:10 - loss: 0.0627 - accuracy:  - ETA: 1:10 - loss: 0.0623 - accuracy: 0. - ETA: 1: - ETA: 1:07 - loss: 0.0617 - accura - ETA: 1:07 - loss: 0.0620 - accuracy: 0.98 - ETA: 1:07 - loss: 0.0620 - accuracy: 0.98 - ETA: 1:07 - loss: 0.0624 - accuracy: 0. - ETA: 1:07 - loss: - ETA: 1:05 - loss: 0.0633 - ac - ETA: 1:05 - loss: 0.0635 - accuracy:  - ETA: 1:05 - loss: 0.0637 -  - ETA: 1:04 - loss: 0.0634 - accuracy: 0.97 - ETA: 1:04 - loss: 0.0 - ETA: 1:03 - loss: 0.0638 - accuracy: 0. - ETA: 1:03 - loss: 0.0642 - ac - ETA: 1:01 - loss: 0.0644 - accuracy: 0.97 - ETA: 1:01 - lo - ETA: 59s - loss: 0.0652 - ETA: 58s - loss: 0.0646 - accur - ETA: 58s - loss: 0.0653 - accuracy: 0. - ETA: 58s - loss: 0.0652 - accuracy: 0.9 - ETA: 58s - loss: 0.0653 - accuracy: 0.9 - ETA: 57s - los - ETA: 56s - loss: 0.0652 -  - ETA: 54s - loss: 0.0653 - accuracy - ETA: 53s - loss: 0.0651 - accuracy:  - ETA: 53s - loss: 0.0654 - accuracy:  - ETA: 52s - loss: 0.065 - ETA: 51s - loss: 0.0654 - accuracy - ETA: 51s - loss: 0.0654 - accuracy:  - ETA: 50s - loss: 0.0653 - accuracy: 0.978 - ETA: 50s - loss: 0.0654 - acc - ETA: 50s - l - ETA: 48s - loss: 0.0650 -  - ETA: 47s - loss: 0.0644 - accuracy: 0  - ETA: 43s - loss: 0.0643 - accuracy: 0.979 - ETA: 43s - loss: 0.0643 -  - ETA:  - ETA: 40s - - ETA: 33s - loss - ETA: 31s - loss: 0.0656 - ETA: 30s - loss: 0.0661 - - ETA: 29s - loss: 0.0663 - accu - ETA: 28s - loss: 0.0664 - accura - ETA: 27s - loss: 0.0664 - accuracy: 0.9 - ETA: 27s - loss: 0.0663 - - ETA: 26s - loss: 0.0661 - accura - ETA: 25s - loss: 0.0659 - accuracy - ETA: 24s - loss: 0.0658 - accuracy: 0.97 - ETA: 24s - loss: 0.0658 - accuracy: 0. - ETA: 24s - loss: 0.0658 - accuracy: 0.97 - ETA: 24s - loss: 0.0658 - accurac - ETA: 23s - loss: 0.0660 - accuracy: - ETA: 23 - ETA: 20s - loss: 0.0666 - accuracy: 0. - ETA: 20s - loss: 0.0666 - accuracy:  - ETA: 20s - loss: 0.0667 - accuracy: 0.9 - ETA: 19s - loss: 0.0667 - ac - ETA: 18s - loss: 0.0668 - accuracy:  - ETA: 18s - loss: 0.0670 - accuracy: 0 - ETA: 18s - loss: 0.0668 - accu - - ETA: 11s - loss: 0.0674 - accuracy: 0.97 - ETA: 11s - loss: 0.0674 - accuracy: 0.978 - ETA: 11s - loss: 0.0674 - ac - ETA: 8s - loss: 0.0675 - accura - ETA: 8s - loss: 0.0676 -  - ETA: 7s - loss: 0.0675 - ac - ETA: 4s - loss: - ETA: 3s - loss: 0.0677 - ac - ETA: 2s - ETA:  - ETA: 0s - loss: 0.0676 - accuracy: 0.\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 98s 2ms/sample - loss: 0.0576 - accuracy: 0.9815 1:49 - loss: 0.0570 - accuracy: 0.98 - ETA: 1:52 - ETA: 1:31 - loss: 0 - ETA: 1:28 - loss: 0 - - ETA: 1:18 - loss: 0 - ETA: 1:16 - l - ETA: 1:15 - ETA: 1:14 - loss: 0.0503 - accuracy:  - ETA: 1:14 - loss: 0. - ETA: 0s - loss: 0.0576 - accuracy:  - ETA: 0s - loss: 0.0576 - accuracy: 0. - ETA: 0s - loss: 0.0576 - accuracy\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0531 - accuracy: 0.9823 1:18 - loss: 0 - ETA: 1: - ET - ETA: 57s  - ETA: 55s - ETA: 51s - loss: 0.0497 - accuracy:  - ETA: 47s - loss: 0.0501 - accuracy: 0.98 - ETA: 46s - loss: 0.0500 - ac - ETA: 45s - loss: 0.0500 - accuracy: 0 - ETA: 45s - loss: 0.0499 - accuracy: 0.983 - ETA: 45s - loss: 0.0499 - accuracy:  - ETA: 45s - loss: 0.0503 - accuracy: 0.983 - ETA: 45s - loss: 0.0503 -  - ETA: 43s - loss: 0.0504 - accuracy: 0.98 - ETA: 43s - loss: 0.0503 - accuracy: 0.98 -  - ETA: 40s - - ETA: 37s - loss: 0.0 - ETA: 35s - loss:  - ETA: 33s - loss: 0.0506 - accur - ETA: 28s - loss: 0.0509 - accuracy:  - ETA: 28s - loss: 0.0508 - accuracy: - ETA: 27s - loss: 0.0510 - accuracy: 0.98 - ETA: 27s - loss: 0.0510 - accuracy: 0.983 - ETA: 27s - loss: 0.0511 - accuracy: 0.98 - ETA - ETA: 13s - loss: 0.0520 - accuracy: - ETA: 12s - loss: 0.0519 - accuracy: 0.98 -  - ETA: 7s - loss: 0.0521 - accura - ETA: 3s - loss: 0.0527 - accu\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 107s 2ms/sample - loss: 0.0461 - accuracy: 0.98531:43 - loss: 0.0390 - accura - ETA: 1:28 - loss: - ETA: 1:27 - los - ETA: 1:25 - loss: 0.0477 - accu - ETA: 1:23 - loss: 0.0476 - accuracy:  - ETA:  - ETA: 1:06 - loss: 0.0464 - accu - ETA: 57s - loss: 0.0465 - accuracy: 0.985 - ETA: 57s - - ETA: 52s - loss: 0.0466 - acc - ETA: 51s -  - ETA: 44s - loss: 0.0466 - accuracy - ETA: 43s - loss: 0.0464 - accuracy: 0.985 - ETA: 43s - loss: 0.0464 - accura - ETA: 42s - loss: 0.0462 - accu - ETA: 41s - loss: 0.0461 - accurac - ETA: 40s - loss: 0.0462 - accuracy: 0.985 - ETA: 40s  - ETA: 34s - loss: 0.0461 -  - ETA: 33s - loss: 0.0461 - accuracy: 0 - ETA: 32s - loss: 0.0461 - accuracy: 0. - ETA: 32s - loss: 0.0462 - accuracy: 0.985 - ETA: 32s - loss: 0.0461 - accuracy: 0.98 - ETA: 32s - loss: 0.0461 -  - ETA: 27s - loss: 0.0458 - accuracy - ETA: 26s - loss: 0.0459 - accuracy: 0 - ETA:  - ETA: 19s - lo\n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0439 - accuracy: 0.9852 1: - ETA: 1:37 - loss: 0 - ETA: 1:38 - l - ETA: 1:30 - loss: 0.0318 - accuracy: 0. - ETA: 1:29 - loss: 0.0311 - accuracy: 0.98 - ETA: 1:29 - loss: 0.0314 - accuracy: 0. - ETA: 1:27 - loss: 0.0318 - accu - ETA: 1:22 - loss: 0 - ETA: 1:16 - loss: 0.0371 - accuracy: 0.98 - ETA: 1:16 - loss: 0.0371 - accuracy: 0.98 - ETA: 1:16 - loss: 0.0383 - accuracy:  - ETA: 1:14 - loss: 0.0378 - accu - ETA: 1:12 - loss: 0.0380  - ETA: 1:09 - loss: 0.0383 - accu - ETA: 1:08 - loss: 0.0381 - accuracy: 0.98 - ETA: 1: - ETA: 1:04 - loss: 0.0384 - ac - ETA: 1:05 - loss: 0.0387 -  - ETA: 1:07 - - ETA: 1:08 - l - ETA: 1:10 - loss: 0.0387 - accura - ETA: 1:11 - loss: 0.0389 - accura - ETA: 1:11 - loss: 0.0385 - accuracy: 0.98 - ETA: 1:11 - loss: 0.0385 - accuracy - ETA: 1:11 - loss: 0.038 - ETA: 1:11 - loss: 0.0391 - accuracy - ETA: 1:11 - l - ETA: 1:11 - los - ETA - ETA: 1:10 - loss: 0.0381 - ac - ETA: 1:09 - loss: 0.0387 - accuracy: 0.98 - ETA: 1:09 - loss: 0.0387 - ac - ETA: 1:09 - loss: 0.0393 - accuracy: 0.98 - ETA: 1:09 - loss: 0.0392  - ETA: 1:08 - loss: 0.0395 - accura - ETA: 1:08 - loss: 0.0400 - accura - ETA: 1:09 - loss: 0.0404  - ETA: 1:08 - loss: 0.0401 - accuracy:  - ETA: 1:08 - ETA: 1:07 - loss: 0.0398 - ac - ETA:  - - ETA: 1:03 - loss: 0.0405 - accuracy:  - ETA: 1:02 - los - ETA: 1:01 - loss: 0.0410 - accuracy: 0. - ETA: 1:01 - loss: 0.0410 - accu - ETA: 1:01 - loss: 0.0409 -  - ETA: 1:01 - loss: - ETA: 1:00 - loss: 0.0407 - accuracy: 0.9 - ETA: 59s - loss: 0.0407 - accuracy: 0 - ETA: 59s - loss: 0.0404 - accuracy: 0.9 - ETA: 59s - loss: 0.0408 - accuracy: 0 - ETA: 58s - loss: 0.04 - ETA: 56s - loss: 0.0412 - accur - ETA: 53s - loss: 0.0422 - acc - ETA: 52s - loss: 0.0428 - accuracy: 0.985 - ETA: 52s - loss: 0.0427 - accuracy - ETA:  - ETA: 49s - loss: 0.0429 - accuracy: 0.9 - ETA: 49s - loss: 0.0431 - ETA: 48s - loss: 0.0427 - accuracy: 0.98 - ETA: 48s - loss: 0.0427 - accuracy: 0. - ETA: 47s - loss: 0.0429 - accur - ETA: 47s - loss: 0 - ETA: 44s - loss: 0.0425 - accuracy: 0.98 - ETA: 44s - loss: 0.0425 - accuracy: 0.985 - ETA: 44s - loss: 0.0425 - accuracy: 0. - ETA: 44s - loss: 0.0424 - accuracy: 0.985 - ETA: 44s - loss: 0.0 - ETA: 42s - loss: 0.0423 - accuracy:  - ETA: 42s - los - ETA: 29s - loss: 0.0434 - accuracy: 0.9 - ETA: 29s - loss: 0.0434 - accuracy: - ETA: 28s - loss: 0.0 - ETA: 25s - loss: 0.0435 - accuracy: 0.9 - ETA: 25s - loss: 0.0435 - accuracy: 0. - ETA: 25s - loss: 0.0436 - accuracy: 0. - ETA: 24s - loss: 0.0437 - accuracy - ETA: 24s - loss: 0.0437 -  - ETA: 23s - loss: 0.0437 - accuracy: 0 - ETA: 22s - loss: 0.0437 - accuracy: 0.9 - ETA: 22s - loss: 0.0438 - accuracy: 0.9 - ETA: 22s - loss: 0.0437 - accuracy: 0.985 - ETA: 22s - loss: 0.0437 - accuracy - ETA: 21s - loss: 0.0437 - accuracy: 0.98 - ETA: 20s - loss: 0.0436 - accuracy: 0.98 - ETA: 20s - loss: 0.0435 - accurac - ETA: 19s - loss: 0.0435 - accuracy: 0.985 - ETA: 19s - loss: 0.0436 - accurac - ETA: 18s - loss: 0.0437 - accuracy - ETA: 18s - loss: 0.0436 - accuracy: 0.98 - ETA: 18s - loss: 0.0436 - accurac - ETA: 17s - loss: 0.0436 - accu - ETA: 16s - loss: 0.0439 - ac - ETA: 14s - loss: 0 - ETA: 12s - loss: 0.0438 - accuracy - ETA: 0s - loss: 0.0438 - accura\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 95s 2ms/sample - loss: 0.0412 - accuracy: 0.9862 1:22 - loss: 0.0355 - accuracy: 0.98 - ETA: 1:22 - loss: 0.0354 -  - ETA:  - ETA: 1:07 - loss: - ETA: 1:03 - loss: 0.0376 - accuracy: 0.98 - ETA: 1:03 - loss: 0.0375 - accuracy: 0.98 - ETA - ET - ETA: 53s - loss: 0.0378 - accuracy:  - ETA: 52s - loss: 0.0377 - ac - ETA: 49s - lo - ETA: 43s - loss: 0.0390 - ac - ETA: 41s - loss: 0.0395 - accuracy: 0.98 - ETA: 41s - loss: 0.0394 - accuracy: 0.98 - ETA: 41s - loss: 0.0393 - accuracy: 0. - ETA: 41s - loss: 0.0393 - accuracy: 0.98 - ETA: 41s - loss: 0.0392 - accuracy: 0.987 - ETA: 41s - loss: 0.0392 - accur - ETA: 40s - loss: 0.03 - ETA: 39s - loss: 0 - ETA: 3 - ETA: 31s - loss: 0.0398 - accuracy: 0 - ETA: 30s - loss: 0.0398 - accu  - ETA: 26s - loss: 0.0406 - accur - ETA: 25s - loss: 0.0407 - accuracy: 0.986 - ETA: 25s - loss: 0.0407 - accuracy: 0.98 - ETA: 25s - loss: 0.040 - ETA: 24s - loss: 0.0405 - accuracy: 0.986 - ETA: 24s - loss: 0.0405 - accuracy: 0.986 - ETA: 24s - loss: 0.0405 - accuracy: 0.986 - ETA: 24s - loss: 0.0405 - accuracy: 0. - ETA: 23s - loss: 0.0405 - a - ETA: 23s - loss: 0.0 - ETA: 21s  - ETA: 19s - loss: 0.0410 - accura - ETA: 18s - loss: 0.0409 - accuracy: 0 - ETA: 18s - loss: 0.0408 - accura - ETA: 17s - loss: 0.04 - ETA: 16s - loss: 0.0417  - ETA: 14s - loss: 0.0416 - accuracy:  - ETA: 14s - loss: 0.0417 - accuracy: 0.9 - ETA: 14s - loss: 0.0417 - accuracy: 0.986 - ETA: 14s - loss: 0.0417 - accuracy: 0 - ETA: 13s - loss: 0.0417 - accur - ETA: - ETA: 3s - - - ETA: 0s - loss: 0.041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ff8b348>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5M60K4iHzed"
   },
   "source": [
    "# STEP 6 : EVALUATE THE TRAINING \n",
    "\n",
    "### Now let’s check that the model performs well on the test set, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FRfnoWkNHp0M",
    "outputId": "b76bb18a-d3d1-4ab6-baa7-43ddf0f97b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/1 - 3s - loss: 0.0398 - accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(x_test,  y_test, verbose=2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e74QmEfYITb7",
    "outputId": "67fe8dc3-5c8f-4c1b-ce32-c863a686793d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07567182519305497\n",
      "0.9785\n"
     ]
    }
   ],
   "source": [
    "print(evaluation[0])   #test loss\n",
    "print(evaluation[1])   #test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZ_YfIkiKBIr"
   },
   "source": [
    "# STEP 7 (optional) : PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "ddDL6T8XJyMQ",
    "outputId": "6217ab4e-6b1e-47e8-f34e-4463436020aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_test[90],cmap='Greys')\n",
    "model.predict_classes(x_test[90].reshape(1, 28, 28 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WbnDC1AGIykE",
    "outputId": "8e9d24da-e3ba-438b-c641-4af23c1bd44f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_test[7].reshape(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VKZB6zz-Juub",
    "outputId": "a22904c2-5dd1-4a9e-fb1d-230c3885fb9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "class1_tf2.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
